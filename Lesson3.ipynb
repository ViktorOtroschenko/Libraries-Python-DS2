{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Построение модели классификации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1.\n",
    "Для чего и в каких случаях полезны различные варианты усреднения для метрик качества классификации: micro, macro, weighted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наиболее часто используемой метрикой для оценки качества мультиклассовой классификации для несбалансированных наборов данных является мультиклассовый вариант f-меры. Идея, лежащая в основе мультиклассовой f-меры, заключается в вычислении одной бинарной f-меры для каждого класса, интересующий класс становится положительным, а все остальные – отрицательными классами. Затем эти f-меры для каждого класса усредняются с использованием одной из следующих стратегий:\n",
    "\n",
    "• __\"macro\"__ усреднение вычисляет f-меры для каждого класса и находит их невзвешенное среднее. Всем классам, независимо от их размера, присваивается одинаковый вес. Полезно использовать, когда необходимо оценить максимальное качество работы классификатора и при этом качество предсказаний миноритарного класса не критично.\n",
    "\n",
    "• __\"weighted\"__ усреднение вычисляет f-меры для каждого класса и находит их среднее, взвешенное по поддержке (количеству фактических примеров для каждого класса). Эта стратегия используется в классификационном отчете по умолчанию. Полезно оценивать, если в наборе данных разным классам присваивается разный вес (значимость).\n",
    "\n",
    "• __\"micro\"__ усреднение вычисляет общее количество ложно положительных примеров, ложно отрицательных примеров и истинно положительных примеров по всем классам, а затем вычисляет точность, полноту и f-меру с помощью этих показателей. Полезно для оценки несбалансированных выборок.\n",
    "\n",
    "Если нам необходимо присвоить одинаковый вес каждому примеру, рекомендуется использовать микро-усреднение f1-меры, если нам\n",
    "необходимо присвоить одинаковый вес каждому классу, рекомендуется использовать макро-усреднение f1-меры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2.\n",
    "В чём разница между моделями xgboost, lightgbm и catboost или какие их основные особенности?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритмы отличаются друг от друга реализацией алгоритма усиленных деревьев, их технической совместимостью и ограничениями. XGBoost был первым, кто попытался улучшить время обучения GBM, за ним последовали LightGBM и CatBoost, каждый со своими собственными методами, в основном связанными с механизмом разделения.\n",
    "\n",
    "__LightGBM__ предлагает одностороннюю выборку на основе градиента (GOSS), которая выбирает разделение, используя все экземпляры с большими градиентами (т. е. с большой ошибкой) и случайную выборку экземпляров с небольшими градиентами. Чтобы сохранить такое же распределение данных при вычислении прироста информации, GOSS вводит постоянный множитель для экземпляров данных с небольшими градиентами. Таким образом, GOSS достигает хорошего баланса между увеличением скорости за счет уменьшения количества экземпляров данных и сохранением точности изученных деревьев решений. Этот метод не является методом по умолчанию для LightGBM, поэтому его следует выбрать явно.\n",
    "    __LightGBM__ использует рост деревьев по листьям (сначала лучший). Он выбирает выращивать лист, который сводит к минимуму потери, позволяя вырасти несбалансированному дереву. Поскольку он растет не по уровням, а по листам, переобучение может произойти при небольшом объеме данных. В этих случаях важно контролировать глубину дерева.\n",
    "\n",
    "__Catboost__ предлагает новую технику под названием Minimal Variance Sampling (MVS), которая представляет собой взвешенную версию Stochastic Gradient Boosting. В этом методе взвешенная выборка происходит на уровне дерева, а не на разделенном уровне. Наблюдения для каждого дерева бустинга отбираются таким образом, чтобы обеспечить максимальную точность оценки разделения.\n",
    "__Catboost__ выращивает сбалансированное дерево. На каждом уровне такого дерева выбирается пара разбиения признаков, которая приводит к наименьшим потерям (согласно штрафной функции), и используется для всех узлов уровня. Его политику можно изменить с помощью параметра grow-policy .\n",
    "\n",
    "\n",
    "__XGboost__ не использует методы взвешенной выборки, что замедляет процесс разделения по сравнению с GOSS и MVS.\n",
    "__XGboost__ разделяется до указанного гиперпараметра max_depth, а затем начинает обрезку дерева в обратном направлении и удаляет разбиения, за пределами которых нет положительного усиления. Он использует этот подход, поскольку иногда за разделением без снижения потерь может следовать разделение с сокращением потерь. XGBoost также может выполнять рост деревьев по листьям (как LightGBM)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
